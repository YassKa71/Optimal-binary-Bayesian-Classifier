\documentclass[12pt,a4paper]{article}


\input{MyPackages.tex}
\input{MyCommandes.tex}

% \usepgfplotslibrary{external}
% \tikzexternalize[prefix=./tikz/,optimize command away=\includepdf,shell escape=-enable-write18]
% \tikzset{external/system call= {pdflatex -save-size=80000 
%                           -pool-size=10000000 
%                           -extra-mem-top=50000000 
%                           -extra-mem-bot=10000000 
%                           -main-memory=90000000 
%                           \tikzexternalcheckshellescape 
%                           -halt-on-error 
%                           -interaction=batchmode
%                           -jobname "\image" "\texsource"}}

%\setBold[0.2] % Active la police grasse 
\title{\textsc{Machine Learning LAB - Optimal binary Bayesian classifer}}
\author{Mahamat HAMID \and Yassine KADDAMI}
%\date{October 2022}

\begin{document}
	\loadgeometry{1}
	\maketitle
	%\tableofcontents
	%\newpage
	\section{Theoretical computations}
	The purpose of this lab is to study the MPE test and assess it with respect to the Neyman-pearson test.
Section 1 is dedicated to the theoretical computations that you are asked in Section 2 to implement so as
to run simulations and verify these theoretical results. All the results needed to write your routines, in
whatever language you wish to use, are given below; in Section 1.2, these results are framed. Hence, it is
suggested that you begin by carrying out the simulations of Section 2 and make the theoretical computations
later. You can return your theoretical computations in latex, word or even as a photo of your hand-writen
notes (if the writing and the presentation are clear).
\subsection{The MPE test and its probability of error}

We consider the binary hypothesis testing problem : 
$$ \begin{cases}
    \mathcal{H}_0 :\,  X \sim \mathcal{N}(0,\sigma^2 \textbf{I}_N) \\
    \mathcal{H}_1 :\,   X \sim \mathcal{N}(\theta,\sigma^2 \textbf{I}_N)
\end{cases} \quad \text{where }\sigma>0 \text{ and } \theta \in \ens{R}^N $$
 We assume the existence of prior probabilities of occurrence $\pi_0$ and $\pi_1$
for $\mathcal{H}_0$ and $\mathcal{H}_1,$ respectively. Alternatively, we can also pose : 
$$X = \varepsilon X_1 + (1-\varepsilon)X_0 $$
where $\varepsilon, X_0$ and $X_1$ are random variables defined in same probability space $(\Omega, \Sigma, \ens{P})$ such that :

\begin{itemize}[label = $\bullet$]
    \item $ X_0 \sim \mathcal{N}(0,\sigma^2 \textbf{I}_N) $ and $X_1 \sim \mathcal{N}(\theta,\sigma^2 \textbf{I}_N)$
    \item  $\varepsilon$ is independant of $X_1$ and  $X_0$
    \item  $\pi_0 = \ens{P}(\varepsilon = 0)$ and $\pi_1 =\ens{P}(\varepsilon = 1)$
\end{itemize}


\begin{tquesto}{}{}
Compute the likelihood ratio $\Lambda = p_1/p_0 $ of the two hypotheses where $p_1$ is the pdf of $X_1$ and $p_0$
that of $X_0$ (see slide 17) (\textbf{2 pts}).
\end{tquesto}

L'expression des densités de probabilités $p_1$ et $p_0$ est la suivante : 
 $$ \begin{cases}
     p_0(x) & = \dfrac{1}{(2\pi)^{N/2}\sqrt{\abs{\det\left( \sigma^2 \textbf{I}_N \right)}}}\exp\left\{ -\dfrac{1}{2\sigma^2} x^t \cdot \textbf{I}_N\cdot x \right\} \\

     p_1(x) & = \dfrac{1}{(2\pi)^{N/2}\sqrt{\abs{\det\left( \sigma^2 \textbf{I}_N \right)}}}\exp\left\{ -\dfrac{1}{2\sigma^2} (x-\theta)^t \cdot \textbf{I}_N\cdot (x-\theta) \right\} 
 \end{cases}$$
Ainsi en faisant le rapport on trouve : 

\begin{align*}
    \Lambda (x) & =  \dfrac{\exp\left\{ -\dfrac{1}{2\sigma^2} (x-\theta)^t \cdot \textbf{I}_N\cdot (x-\theta) \right\} } {\exp\left\{ -\dfrac{1}{2\sigma^2} x^t \cdot \textbf{I}_N\cdot x \right\}} \\ 
    & = \dfrac{\exp\left\{ -\dfrac{1}{2\sigma^2}\norm{x-\theta}^2 \right\}}{\exp\left\{ -\dfrac{1}{2\sigma^2}\norm{x}^2 \right\}} \\
    & = \exp\left\{  -\dfrac{1}{2\sigma^2}\left(\norm{x-\theta}^2 - \norm{x}^2\right) \right\} = \exp\left\{  -\dfrac{1}{2\sigma^2}\left(\cancel{\norm{x}^2} + \norm{\theta}^2 - 2x^t\cdot\theta - \cancel{\norm{x}^2}\right) \right\}
\end{align*}
 D'où : $$     \colorboxed{red}{\Lambda (x) =  \exp\left\{ \dfrac{1}{\sigma^2}\left(x^t\cdot \theta - \dfrac{\norm{\theta}^2}{2} \right)\right\}}$$

\begin{tquesto}{}{}
Show that the MPE classifier (see slides 16 and 17) is given by :
$$ \forall x \in \ens{R}^N,\quad g_\text{MPE}(x) = \begin{cases}
    1 &\text{ if } x^t\cdot\theta >\sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2 \\
    0 & \text{ otherwise } 
\end{cases} $$
(\textbf{2 pts})
\end{tquesto}

 \begin{align*}
\pi_1 p_1(x) > \pi_0 p_0(x) & \Longleftrightarrow \dfrac{p_1}{p_0}(x) > \dfrac{\pi_0}{\pi_1} \quad\text{ si }\pi_1, p_0(x) \text{ non nuls} \\
 & \Longleftrightarrow \Lambda(x) > \dfrac{\pi_0}{\pi_1} \\
 & \Longleftrightarrow \exp\left\{ \dfrac{1}{\sigma^2}\left(x^t\cdot \theta - \dfrac{\norm{\theta}^2}{2} \right)\right\} > \dfrac{\pi_0}{\pi_1} \\
 \pi_1 p_1(x) > \pi_0 p_0(x) & \Longleftrightarrow x^t\cdot\theta >\sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2
 \end{align*}

 Ainsi le test $g_\text{MPE}$ defini par : 
 $$  \forall x \in \ens{R}^N,\quad g_\text{MPE}(x) = \begin{cases}
    1 &\text{ if } \Lambda(x) > \pi_0/\pi_1  \\
    0 & \text{ otherwise } \end{cases} $$

s'écrit comme suit :
$$ \forall x \in \ens{R}^N,\quad g_\text{MPE}(x) = \begin{cases}
    1 &\text{ if } x^t\cdot\theta >\sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2 \\
    0 & \text{ otherwise } 
\end{cases} $$


\begin{tquesto}{}{}
Show that the probability of error of the MPE test (see slides p.15) is:

$$ \ens{P}_e (g_\text{MPE}) =   \pi_0 \left( 1-\Phi \left( \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} + \dfrac{\rho}{2}\right) \right) +\pi_1 \Phi \left( \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} - \dfrac{\rho}{2}\right) $$
with $\rho = \norm{\theta}/\sigma$ and $\Phi$ is the cumultaive distribution function (cdf) of the normal distribu-
tion $\mathcal{N}(0,1)$.\\
(\textbf{4 pts})
\end{tquesto}
Montrons d'abord que:
\begin{align*}
	\colorboxed{red}{\text{Si} \quad Z \sim \mathcal{N}(a\theta, \sigma^2\mathbf{I}_N) \quad \text{alors} \quad Z^T\theta \sim \mathcal{N}(a\|\theta\|^2, \|\theta\|^2\sigma^2) \quad (\ast)}
\end{align*}
Soit $ Z \sim \mathcal{N}(a\theta, \sigma^2\mathbf{I}_N) $.  $ Z^T\theta $ est gaussien puisque c'est une combinaison linéaire d'éléments d'un vecteur gaussien.\\
Il suffit donc de montrer que :
$\begin{cases}
	\mathbb{E}[Z^T\theta] = a\|\theta\|^2 \\
	\mathbb{V}[Z^T\theta] = \|\theta\|^2\sigma^2 
\end{cases}$\\
$$
	\mathbb{E}[Z^T\theta] = \mathbb{E}\left[\sum_{i=1}^{N} Z_{i}\theta_{i}\right] = \sum_{i=1}^{N} \theta_{i} \mathbb{E}[Z_{i}] = \sum_{i=1}^{N} a\theta_{i}^2 = a\|\theta\|^2 $$
\begin{align*}
	\mathbb{V}[Z^T\theta] & =
	 \mathbb{E}[(Z^T\theta - a\|\theta\|^2)^2] \\ & =\mathbb{E}[(\sum_{i=1}^{N} (Z_{i}\theta_{i} - a\theta_{i}^2))^2] \\ & =
	 \mathbb{E}[(\sum_{i=1}^{N} (Z_{i}\theta_{i} - a\theta_{i}^2))(\sum_{j=1}^{N} (Z_{j}\theta_{j} - a\theta_{j}^2))]  \\ & =
	  \mathbb{E}[(\sum_{i=1}^{N} \sum_{j=1}^{N} (Z_{i}\theta_{i} - a\theta_{i}^2)(Z_{j}\theta_{j} - a\theta_{j}^2))]\\ & =
	  \sum_{i=1}^{N} \sum_{j=1}^{N} \theta_{i}\theta_{j}\mathbb{E}[( (Z_{i} - a\theta_{i})(Z_{j} - a\theta_{j}))]
\end{align*}
Puisque la matrice de covariance de Z vaut $ \sigma^2\mathbf{I}_N $, on obtient:
$$ 
	\mathbb{V}[Z^T\theta] = \sum_{i=1}^{N} \sum_{j=1}^{N} \theta_{i}\theta_{j}\sigma^2\delta_{ij} =\sigma^2 \sum_{i=1}^{N} \theta_{i}^2 = \sigma^2\|\theta\|^2 $$
On peut maintenant répondre à la question on utilisant le résultat démontré ci-dessus.
Par définition, la probabilité d'erreur est donnée par : 
$$
    \ens{P}_e\left(g_\text{MPE}(X)\right)  = \pi_0 \ens{P}\left(g_\text{MPE}(X_0) = 1 \right) + \pi_1 \ens{P}\left(g_\text{MPE}(X_1) = 0 \right) $$
Donc 

\begin{align*}
    \ens{P}_e\left(g_\text{MPE}(X)\right) & =
    \pi_0 \ens{P}\left(X_0^t\cdot\theta >\sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2 \right) + \pi_1 \ens{P}\left(X_1^t\cdot\theta 
\leq \sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2 \right) 
\end{align*}
Or d'après ($\ast$): $\begin{cases}
    X_0^t\cdot\theta \sim \mathcal{N}(0,\sigma^2\norm{\theta}^2) \\
    X_1^t\cdot\theta \sim \mathcal{N}(\norm{\theta}^2,\sigma^2\norm{\theta}^2)
\end{cases} \Longrightarrow \begin{cases}
    \dfrac{X_0^t\cdot\theta}{\sigma\norm{\theta}}  &\sim \mathcal{N}(0,1) \\
    \dfrac{X_1^t\cdot\theta- \norm{\theta}^2}{\sigma\norm{\theta}} &\sim \mathcal{N}(0,1)
\end{cases}$\\
Donc :
$$
\begin{cases}
    X_0^t\cdot\theta >\sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2 \\
    X_1^t\cdot\theta 
\leq \sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2
\end{cases} \Longleftrightarrow \begin{cases}
    \dfrac{X_0^t\cdot\theta}{\sigma\norm{\theta}} > \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} + \dfrac{\rho}{2} \\
    \dfrac{X_1^t\cdot\theta -\norm{\theta}^2}{\sigma\norm{\theta}}
\leq \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} - \dfrac{\rho}{2}
\end{cases}
$$
Donc : 
$$\begin{cases}
    \ens{P}\left(X_0^t\cdot\theta >\sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2 \right) & = \ens{P}\left(\dfrac{X_0^t\cdot\theta}{\sigma\norm{\theta}} > \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} + \dfrac{\rho}{2}\right) = 1-\Phi \left( \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} + \dfrac{\rho}{2}\right)  \\

    \ens{P}\left(X_1^t\cdot\theta 
\leq \sigma^2\ln(\pi_0/\pi_1) + \norm{\theta}^2/2 \right) & = \ens{P}\left(\dfrac{X_1^t\cdot\theta -\norm{\theta}^2}{\sigma\norm{\theta}}
\leq \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} - \dfrac{\rho}{2}  \right)  = \Phi \left( \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} -\dfrac{\rho}{2}\right)
\end{cases}
$$
Ce qui donne finalement : 
$$  \colorboxed{red}{\ens{P}_e (g_\text{MPE}) =   \pi_0 \left( 1-\Phi \left( \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} + \dfrac{\rho}{2}\right) \right) +\pi_1 \Phi \left( \dfrac{1}{\rho}\ln\dfrac{\pi_0}{\pi_1} - \dfrac{\rho}{2}\right)} $$


\subsection{A detour by the Neyman-Pearson theory}
In this section, we apply the Neymal-Pearson (NP) test to the classification problem considered so far.
We make the computations to state the results that you are asked to use in the next section to carry
out simulations. It is recommended that you take some time at home to fully understand the following
reasoning and calculations. For the lab session, admit the formulas given below and try to proce them
later. They are not so difficult to prove and if necessary contact me for further explanations.
According to your course on statistics, the NP test $g^\gamma_\text{NP}$ with size $\gamma \in ]0,1[$  to test $\mathcal{H}_0$ against $\mathcal{H}_1$
when we ignore the priors $\pi_0$ and $\pi_1$ is given by

$$ \forall x\in \ens{R}^N,\quad g_\text{NP}^\gamma(x) = \begin{cases}
    1 & \text{ if } \Lambda (x) > \lambda \\
    0 & \text{ otherwise}
\end{cases} $$
where, as above, $\Lambda$ is the likelihood ration and $\lambda$ satisfies the equation $\ens{P}(\Lambda(X_0) > \lambda) = \gamma$


\begin{tquesto}{}{}
Prove the inequality $\forall A>0,\, \forall \gamma \in ]0;1[, \quad 1- \Phi(A/2) \leqslant \num{.5}\cdot\left(\gamma + \Phi\left(\Phi^{-1}(1-\gamma)-A\right)\right)$
(\textbf{1 pt}).
\end{tquesto}
Considérons le cas où les probabilités à priori sont telles que :
$\pi_0 = \pi_1 = 1/2$. Par définition du MPE, on a que :
\begin{equation}
 \ens{P}_e(g_\text{MPE}) \leqslant \ens{P}_e(g) \label{MPE} \tag{$*$}   
\end{equation}
et ce $\forall g \in \mathcal{F}(\ens{R}^N, \{0,1\}).$\\
Soit $A,\gamma \in \ens{R}_+^*\times ]0;1[$ tels que $A = \rho = \norm{\theta}/\sigma $ et $\gamma$ défini une pfa comme ci-dessus. D'après \eqref{MPE}, en posant $g = g_\text{NP}^\gamma$ on a : 
\begin{align*}
  \ens{P}_e(g_\text{MPE}) \leqslant \ens{P}_e(g_\text{NP}^\gamma) & \Longleftrightarrow  \dfrac{1}{2}\left(1-\Phi\left(\dfrac{A}{2}\right)\right) + \dfrac{1}{2}\Phi\left(-\dfrac{A}{2}\right) \leqslant \dfrac{1}{2}\cdot\left(\gamma + \Phi\left(\Phi^{-1}(1-\gamma)-A\right)\right) \\
  & \intertext{Or :  $\Phi(-x) = 1-\Phi(x)$}\\
  & \Longleftrightarrow \dfrac{1}{2}\left(1-\Phi\left(\dfrac{A}{2}\right)\right) + \dfrac{1}{2}\left(1-\Phi\left(\dfrac{A}{2}\right)\right) \leqslant \dfrac{1}{2}\cdot\left(\gamma + \Phi\left(\Phi^{-1}(1-\gamma)-A\right)\right)
\end{align*}
D'où 
$$  1- \Phi\left(\dfrac{A}{2}\right) \leqslant \frac{1}{2}\cdot\left(\gamma + \Phi\left(\Phi^{-1}(1-\gamma)-A\right)\right) \qquad \forall A \in \ens{R}_+^*, \gamma \in ]0,1[. $$

\section{Numerical simulations}
The purpose of these numerical simulations is to verify numerically the theoretical results stated above for the MPE and the NP tests.\\
\begin{tquesto}{}{}
	1. Write a function with input parameters $M$, $\pi_1$, $\theta$, $N$ to generate $M$ realizations of $X$ as defined in (1).\\
	2. Calculate the error rate of the MPE test when $N = 2$, $\theta = A\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)$, $A$ varies in $[1, 10]$, $\pi_1 = 0.2$, $\sigma = 1$. Choose $M$ at will but don't choose a too small value, otherwise, your simulations will not fit well the theoretical results.\\
	3. Calculate the error rate of the Neyman-Pearson test with size $\gamma = 10^{-3}$ when, as above, $N = 2$, $\theta = A\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)$, $A$ varies in $[1, 10]$, $\pi_1 = 0.2$.\\
	4. Calculate numerically the probabilities of error of the MPE test and the NP with size $\gamma = 10^{-3}$ (cf. questions 2 and (4)) when, as above, $N = 2$, $\theta = A\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)$, $A$ varies in $[1, 10]$, and $\pi_1 = 0.2$.\\
	5. On the same figure, plot the error rates and probabilities of error of these tests to verify that the theoretical results are experimentally verified by your Monte-Carlo simulations and compare the performance of the MPE and NP tests. In particular, verify numerically the inequality stated in question 4.\\
	6. Can you explain the difference in performance of the Neyman-Pearson and Bayesian classifiers?
\end{tquesto}

\begin{figure}[h!]
	\centering
	\includegraphics{Error_rate.pdf}
\end{figure}

D'après la figure obtenue ci-dessous, on voit que le test MPE a tendance à avoir un taux d'erreur et une probabilité d'erreur plus faibles par rapport au test NP. Dans le cadre de notre problème, le test MPE semble être le meilleur choix pour réduire les erreurs de classification.\\

6. On peut expliquer cette différence de performances par le fait que le test MPE cherche à maximiser la probabilité a posteriori pour choisir l'hypothèse la plus probable compte tenu des données et des probabilités a priori. En revanche, le test NP vise à contrôler strictement le taux d'erreur de Type I (faux positifs) avec le seuil tout en maximisant la puissance du test.
\end{document}
